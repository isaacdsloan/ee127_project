{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib.colors import LogNorm\n",
    "from matplotlib import animation\n",
    "\n",
    "from scipy.optimize import minimize, OptimizeResult\n",
    "from collections import defaultdict\n",
    "from itertools import zip_longest\n",
    "from functools import partial\n",
    "\n",
    "from answer import Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Implementation (Students do)</span>\n",
    "<hr/>\n",
    "\n",
    "### Methods\n",
    "You will implement five optimization algorithms (descriptions available [here](https://ruder.io/optimizing-gradient-descent/index.html)). \n",
    "- Gradient descent (`gd`)\n",
    "- Momentum gradient method (`momentum`)\n",
    "- Nesterov's accelerated gradient method (`nag`)\n",
    "- Adaptive gradient method (`adagrad`)\n",
    "- Adaptive moment estimation (`adam`)\n",
    "\n",
    "The last is a very common optimizer used in practical applications -- possibly the most common in the world.\n",
    "\n",
    "Make note of the function headers: `def gd(func, x, lr, num_iters, jac, tol, callback, *args, **kwargs):`. Each method will satisfy this header format in accordance with the specification of custom minimizers used with `scipy.optimize.minimize`. This function is [well-documented](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html), but the highlights of the arguments are below.\n",
    "- `func`: [type: function] The loss function. Takes in a point of type np.ndarray (2,) and returns a float representing the value of the function at that point.\n",
    "- `x`: [type: np.ndarray (2,)] The starting point of the optimization.\n",
    "- `lr`: [type: function] Learning rate schedule. Takes in an argument of type int representing the iteration number, and returns the learning rate to be used for that iteration.\n",
    "- `num_iters`: [type: int] The number of iterations of the optimization method to run.\n",
    "- `jac`: [type: function] The gradient of the loss function. \"Jac\" stands for Jacobian, which is out of scope for this class, but for scalar-valued functions, it is the transpose of the gradient. Takes in a point of type np.ndarray (2,) and returns an np.ndarray (2,) representing the gradient of the function at that point.\n",
    "- `tol`: [type: float] The tolerance within which the optimization method is deemed to have converged.\n",
    "- `callback`: [type: function] A function to be called on each iterate over the course of the optimization.\n",
    "- `*args` and `**kwargs`: You will not need to use these, but they are present for compatibility with the `scipy.optimize.minimize` API.\n",
    "\n",
    "Each function will need to return a two-tuple containing\n",
    "- An instance of `scipy.optimize.OptimizeResult`, described [here](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.OptimizeResult.html).\n",
    "- A `np.ndarray` containing the function value at the initial point and each iterate over the course of the optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " x: array([0, 0])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# demonstration of scipy.optimize.minimize\n",
    "# minimize(lambda x: (x ** 2, 2 * x), 3, jac=True, method='Newton-CG', callback=print)\n",
    "OptimizeResult(x=np.array([0,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0. , 0.5, 1. ])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.ndarray(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gd(func, x, lr, num_iters, jac, tol, callback, *args, **kwargs):\n",
    "    # What is callback for?\n",
    "    func_values = [func(x)]\n",
    "    x_values = [x]\n",
    "    for i in np.arange(num_iters):\n",
    "        x_next = x_values[i] - lr(i) * jac(x_values[i])\n",
    "        func_values.append(func(x_next))\n",
    "        x_values.append(x_next)\n",
    "        # x_values has added index\n",
    "        print(np.array(x_values[i+1]))\n",
    "        if np.linalg.norm(x_values[i+1] - x_values[i]) < tol:\n",
    "            opt_result = OptimizeResult(x=x_values[-1], success=True, message=\"Converged\", nit=1+i)\n",
    "            return (opt_result, np.array(func_values))\n",
    "    \n",
    "    opt_result = OptimizeResult(x=x_values[-1], success=False, message=\"Failed to converge\", nit=num_iters)\n",
    "    return (opt_result, np.array(func_values))\n",
    "    \n",
    "    \n",
    "\n",
    "def momentum(func, x, lr, num_iters, jac, tol, callback, gamma=0.9, *args, **kwargs):\n",
    "    v_values = [0]\n",
    "    func_values = [func(x)]\n",
    "    x_values = [x]\n",
    "    for i in np.arange(num_iters):\n",
    "        v_next = gamma * v_values[i] + lr(i) * jac(x_values[i])\n",
    "        x_next = x_values[i] - v_next\n",
    "        func_values.append(func(x_next))\n",
    "        x_values.append(x_next)\n",
    "        v_values.append(v_next)\n",
    "        # x_values has added index\n",
    "        print(np.array(x_values[i+1]))\n",
    "        if np.linalg.norm(x_values[i+1] - x_values[i]) < tol:\n",
    "            opt_result = OptimizeResult(x=x_values[-1], success=True, message=\"Converged\", nit=1+i)\n",
    "            return (opt_result, np.array(func_values))\n",
    "    \n",
    "    opt_result = OptimizeResult(x=x_values[-1], success=False, message=\"Failed to converge\", nit=num_iters)\n",
    "    return (opt_result, np.array(func_values))\n",
    "    \n",
    "\n",
    "def nag(func, x, lr, num_iters, jac, tol, callback, gamma=0.9, *args, **kwargs):\n",
    "    v_values = [0]\n",
    "    func_values = [func(x)]\n",
    "    x_values = [x]\n",
    "    for i in np.arange(num_iters):\n",
    "        v_next = gamma * v_values[i] + lr(i) * jac(x_values[i] - gamma * v_values[i])\n",
    "        x_next = x_values[i] - v_next\n",
    "        func_values.append(func(x_next))\n",
    "        x_values.append(x_next)\n",
    "        v_values.append(v_next)\n",
    "        # x_values has added index\n",
    "        print(np.array(x_values[i+1]))\n",
    "        if np.linalg.norm(x_values[i+1] - x_values[i]) < tol:\n",
    "            opt_result = OptimizeResult(x=x_values[-1], success=True, message=\"Converged\", nit=1+i)\n",
    "            return (opt_result, np.array(func_values))\n",
    "    \n",
    "    opt_result = OptimizeResult(x=x_values[-1], success=False, message=\"Failed to converge\", nit=num_iters)\n",
    "    return (opt_result, np.array(func_values))\n",
    "    \n",
    "\n",
    "def adagrad(func, x, lr, num_iters, jac, tol, callback, eps=1e-5, *args, **kwargs):\n",
    "    \n",
    "    pass\n",
    "\n",
    "def adam(func, x, lr, num_iters, jac, tol, callback, beta1=0.9, beta2=0.999, eps=1e-5, *args, **kwargs):\n",
    "    pass\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions and gradients\n",
    "You have been given the implementation of four functions ($\\mathbb{R}^2\\to\\mathbb{R}$), given below. You will need to implement `grad`, which returns their gradients as `np.ndarray` (2,). There is a field below for you to submit the gradient in $\\LaTeX$.\n",
    "- Booth function: $f_1(x)=\\left(x_1+2x_2-7\\right)^2+\\left(2x_1+x_2-5\\right)^2$\n",
    "- Beale function: $f_2(x)=\\left(1.5-x_1+x_1 x_2\\right)^2+\\left(2.25-x_1+x_1x_2^2\\right)^2+\\left(2.625-x_1+x_1 x_2^3\\right)^2$\n",
    "- Rosenbrock function: $f_3(x)=100\\cdot\\left(x_2-x_1^2\\right)^2+\\left(x_1-1\\right)^2$\n",
    "- Ackley function: $f_4(x)=-20\\cdot\\exp\\left(-\\frac{1}{5}\\sqrt{\\frac{x_1^2+x_2^2}{2}}\\right)-\\exp\\left(\\frac{\\cos 2\\pi x_1 + \\cos 2\\pi x_2}{2}\\right)+20+\\exp(1)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func(fn, x, y):\n",
    "    if fn == 'booth':\n",
    "        return (x + 2*y - 7)**2 + (2*x + y -5)**2\n",
    "    elif fn == 'beale':\n",
    "        return 0\n",
    "    elif fn == 'rosen2d':\n",
    "        return 0\n",
    "    elif fn == 'ackley2d':\n",
    "        return 0\n",
    "    else:\n",
    "        raise ValueError('Function %s not supported.' % fn)\n",
    "        \n",
    "def grad(fn, x, y):\n",
    "    gradient = np.zeros_like(x)\n",
    "    if fn == 'booth':\n",
    "        g1 = 10*x + 8*y - 34\n",
    "        g2 = 8*x + 10*y - 38\n",
    "        np.array([g1, g2])\n",
    "    elif fn == 'beale':\n",
    "        g1 = 0\n",
    "        g2 = 0\n",
    "    elif fn == 'rosen2d':\n",
    "        g1 = 0\n",
    "        g2 = 0\n",
    "    elif fn == 'ackley2d':\n",
    "        g1 = 0\n",
    "        g2 = 0\n",
    "    else:\n",
    "        raise ValueError('Function %s not supported.' % fn)\n",
    "    return np.stack((g1, g2), axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:red\">Submission: Gradient values $\\LaTeX$</span>\n",
    "<div style=\"color: red\">\n",
    "Enter the gradients you calculated below.\n",
    "<ul>\n",
    "<li>Booth: $\\nabla_x f_1(x)=\\text{replace me}$</li>\n",
    "<li>Beale: $\\nabla_x f_2(x)=\\text{replace me}$</li>\n",
    "<li>Rosenbrock: $\\nabla_x f_3(x)=\\text{replace me}$</li>\n",
    "<li>Ackley: $\\nabla_x f_4(x)=\\text{replace me}$</li>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Student-facing `Answer` class (provided)</span>\n",
    "\n",
    "You have been provided a class called `Answer` which will be helpful for the remainder of the project. It can be found in the `answer.py` file. You are welcome to read and modify it, but this is not required. All information you need about this class is documented here, and examples of usage are given below.\n",
    "\n",
    "### Documentation\n",
    "- `__init__(self, methods, func, grad)`\n",
    "  - Instantiates the `Answer` class with the functions you have implemented. `methods` is a dictionary mapping algorithm names to the functions that implement them, and `func` and `grad` are the functions of the same name that you have implemented.\n",
    "- `set_fn_settings(self, fn_name)`\n",
    "  - Sets the instance variables needed for visualizing `fn_name` with `plot2d` and `plot3d`. Needs to be called before calling these functions.\n",
    "- `set_settings(self, fn_name, method, x0, **kwargs)`\n",
    "  - Sets the instance variables needed for visualizing `method` optimizing `fn_name` starting at `x0` with `path2d`, `path3d`, `video2d`, and `video3d`. Any additional `kwargs` (likely `lr` and `num_iters`) will be passed on to `method`. Needs to be called before calling these functions or `compare`.\n",
    "- `get_settings(self)`\n",
    "  - Returns the arguments passed into `set_settings`: `fn_name`, `method`, `x0`, and `kwargs`.\n",
    "- `compare(self, method, start_iter=0, **kwargs)`\n",
    "  - Generates training loss graph comparing `method` with the previously set method on the previously set loss function and starting point, starting at iteration `start_iter`. Additional `kwargs` (likely `lr` and `num_iters`) will be passed on to `method`.\n",
    "- `get_xs_losses(self)`\n",
    "  - Returns a tuple containing\n",
    "    - [type: `np.ndarray` (1 + `n_iters`, 2)] All iterates (including the initial point).\n",
    "    - [type: `np.ndarray` (1 + `n_iters`,)] The loss at each iterate.\n",
    "- `get_min_errs(self)`\n",
    "  - Returns a tuple containing\n",
    "    - `float` representing the closest (in L2 norm) the optimization procedure got to the global minimizer.\n",
    "    - `float` representing the closest the optimization procedure got to the global minimum function value.\n",
    "- `func_val(self, x)`\n",
    "  - Returns `float` value of the previously set loss function evaluated at `x`. Convenience tool for debugging.\n",
    "- `grad_val(self, x)`\n",
    "  - Returns `np.ndarray` (2,) gradient of the previously set loss function evaluated at `x`. Convenience tool for debugging.\n",
    "- `plot2d(self)`\n",
    "  - Plots contours of the previously set loss function.\n",
    "- `plot3d(self)`\n",
    "  - Plots the previously set loss function.\n",
    "- `path2d(self)`\n",
    "  - Plots the sequence of iterates produced by the set method on the set loss function on a 2D contour.\n",
    "- `path3d(self)`\n",
    "  - Plots the sequence of iterates produced by the set method on the set loss function on a 3D graph. **NOTE:** This one does not work very well.\n",
    "- `video2d(self, filename=None)`\n",
    "  - Creates and saves an MP4 video of the path taken in `path2d` at `filename`. File name defaults to \"{function}_{method}_2d.mp4\"\n",
    "- `video3d(self, filename=None)`\n",
    "  - Creates and saves an MP4 video of the path taken in `path3d` at `filename`. File name defaults to \"{function}_{method}_3d.mp4\". **NOTE:** This works better than `path3d`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the Answer class with the methods you have implemented! (You can implement and add more if you like!)\n",
    "ans = Answer(\n",
    "    {  # a mapping of algorithm names to functions implementing them\n",
    "        'gd': gd,\n",
    "        'momentum': momentum,\n",
    "        'nag': nag,\n",
    "        'adagrad': adagrad,\n",
    "        'adam': adam\n",
    "    },\n",
    "    func,\n",
    "    grad\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing your code\n",
    "We are not providing much structure here, but now is a good time to make sure your optimization methods are working well. The cell below tests your gradient descent method on the function $f(x)=x^2$. We have included the output of our solution as a comment. Note that the function you feed it needs to take in a point as its sole argument and return the function as well as the gradient evaluated at that point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5\n",
      "0.75\n",
      "0.375\n",
      "0.1875\n",
      "0.09375\n",
      "0.046875\n",
      "0.0234375\n",
      "0.01171875\n",
      "0.005859375\n",
      "0.0029296875\n",
      "0.00146484375\n",
      "0.000732421875\n",
      "Final iterate: 0.000732. Number of iterations: 12. Final loss: 0.00000054.\n"
     ]
    }
   ],
   "source": [
    "# Maybe a useful starting example for testing gradient descent on a simple function\n",
    "x_squared = lambda x: (x**2, 2*x)  # returns both the function value and the gradient.\n",
    "opt_res, losses = minimize(x_squared, 3, jac=True, method=gd, callback=print,\n",
    "                           options=dict(lr=lambda t: 0.25, x0=3, num_iters=15, tol=1e-3))\n",
    "print('Final iterate: %.6f. Number of iterations: %d. Final loss: %.8f.' % (opt_res.x, opt_res.nit, losses[-1]))\n",
    "# ----------------\n",
    "# Expected output (GD):\n",
    "# ----------------\n",
    "# [1.5]\n",
    "# [0.75]\n",
    "# [0.375]\n",
    "# [0.1875]\n",
    "# [0.09375]\n",
    "# [0.046875]\n",
    "# [0.0234375]\n",
    "# [0.01171875]\n",
    "# [0.00585938]\n",
    "# [0.00292969]\n",
    "# [0.00146484]\n",
    "# [0.00073242]\n",
    "# Final iterate: 0.000732. Number of iterations: 12. Final loss: 0.00000054."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2800000000000002\n",
      "1.0848000000000002\n",
      "-0.2512319999999997\n",
      "-1.3933651199999997\n",
      "-2.0868772991999998\n",
      "-2.2101877086719997\n",
      "-1.79072202711552\n",
      "-0.9834296272069634\n",
      "-0.02084335675959126\n",
      "0.8504866922653456\n",
      "1.4305669302441057\n",
      "1.6093030811664046\n",
      "1.3839328775165365\n",
      "0.8489558036276865\n",
      "0.16372704425707685\n",
      "Final iterate: 0.163727. Number of iterations: 15. Final loss: 0.02680655.\n"
     ]
    }
   ],
   "source": [
    "x_squared = lambda x: (x**2, 2*x)  # returns both the function value and the gradient.\n",
    "opt_res, losses = minimize(x_squared, 3, jac=True, method=momentum, callback=print,\n",
    "                           options=dict(lr=lambda t: 0.12, x0=3, num_iters=15, tol=1e-10))\n",
    "print('Final iterate: %.6f. Number of iterations: %d. Final loss: %.8f.' % (opt_res.x, opt_res.nit, losses[-1]))\n",
    "# ----------------\n",
    "# Expected output (Momentum):\n",
    "# ----------------\n",
    "# [2.28]\n",
    "# [1.0848]\n",
    "# [-0.251232]\n",
    "# [-1.39336512]\n",
    "# [-2.0868773]\n",
    "# [-2.21018771]\n",
    "# [-1.79072203]\n",
    "# [-0.98342963]\n",
    "# [-0.02084336]\n",
    "# [0.85048669]\n",
    "# [1.43056693]\n",
    "# [1.60930308]\n",
    "# [1.38393288]\n",
    "# [0.8489558]\n",
    "# [0.16372704]\n",
    "# Final iterate: 0.163727. Number of iterations: 15. Final loss: 0.02680655."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5999999999999996\n",
      "-0.31200000000000006\n",
      "-0.22655999999999987\n",
      "-0.029932799999999926\n",
      "0.029406336000000005\n",
      "0.016562311679999983\n",
      "0.0010005379583999923\n",
      "-0.0026010116782079987\n",
      "-0.0011684812702310375\n",
      "2.41592193896455e-05\n",
      "0.00021950713200965199\n",
      "Final iterate: 0.000220. Number of iterations: 11. Final loss: 0.00000005.\n"
     ]
    }
   ],
   "source": [
    "x_squared = lambda x: (x**2, 2*x)  # returns both the function value and the gradient.\n",
    "opt_res, losses = minimize(x_squared, 3, jac=True, method=nag, callback=print,\n",
    "                           options=dict(lr=lambda t: 0.4, x0=3, num_iters=15, tol=1e-3))\n",
    "print('Final iterate: %.6f. Number of iterations: %d. Final loss: %.8f.' % (opt_res.x, opt_res.nit, losses[-1]))\n",
    "# ----------------\n",
    "# Expected output (NAG):\n",
    "# ----------------\n",
    "# [0.6]\n",
    "# [-0.312]\n",
    "# [-0.22656]\n",
    "# [-0.0299328]\n",
    "# [0.02940634]\n",
    "# [0.01656231]\n",
    "# [0.00100054]\n",
    "# [-0.00260101]\n",
    "# [-0.00116848]\n",
    "# [2.41592194e-05]\n",
    "# [0.00021951]\n",
    "# Final iterate: 0.000220. Number of iterations: 11. Final loss: 0.00000005."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_squared = lambda x: (x**2, 2*x)  # returns both the function value and the gradient.\n",
    "opt_res, losses = minimize(x_squared, 3, jac=True, method=adagrad, callback=print,\n",
    "                           options=dict(lr=lambda t: 2, x0=3, num_iters=15, tol=1e-3))\n",
    "print('Final iterate: %.6f. Number of iterations: %d. Final loss: %.8f.' % (opt_res.x, opt_res.nit, losses[-1]))\n",
    "# ----------------\n",
    "# Expected output (Adagrad):\n",
    "# ----------------\n",
    "# [1.00000028]\n",
    "# [0.36754467]\n",
    "# [0.13664342]\n",
    "# [0.05087939]\n",
    "# [0.01894909]\n",
    "# [0.00705745]\n",
    "# [0.00262851]\n",
    "# [0.00097897]\n",
    "# [0.00036461]\n",
    "# Final iterate: 0.000365. Number of iterations: 9. Final loss: 0.00000013."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_squared = lambda x: (x**2, 2*x)  # returns both the function value and the gradient.\n",
    "opt_res, losses = minimize(x_squared, 3, jac=True, method=adam, callback=print,\n",
    "                           options=dict(lr=lambda t: 2, x0=3, num_iters=15, tol=1e-3))\n",
    "print('Final iterate: %.6f. Number of iterations: %d. Final loss: %.8f.' % (opt_res.x, opt_res.nit, losses[-1]))\n",
    "# ----------------\n",
    "# Expected output (Adam):\n",
    "# ----------------\n",
    "# [1.00000333]\n",
    "# [-0.74212166]\n",
    "# [-1.76094027]\n",
    "# [-1.93985392]\n",
    "# [-1.565932]\n",
    "# [-0.89684359]\n",
    "# [-0.12388967]\n",
    "# [0.58433522]\n",
    "# [1.08297399]\n",
    "# [1.292603]\n",
    "# [1.21773996]\n",
    "# [0.91871601]\n",
    "# [0.47982495]\n",
    "# [-0.00561154]\n",
    "# [-0.44385421]\n",
    "# Final iterate: -0.443854. Number of iterations: 15. Final loss: 0.19700656."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Playground and Exploration\n",
    "\n",
    "You are free to use the functions described above to explore the behavior of the optimization algorithms you have implemented. Pick different starting points, learning rate schedules, and even tolerances to explore! Example usage of the `Answer` class is below.\n",
    "\n",
    "### Exploration\n",
    "For each of the functions, start at the given initial points ($x_0$) and use any choice of optimization algorithm and associated hyperparameters to get within the specified distance of the global minimizer and minimum ($\\epsilon_x$, $\\epsilon_f$). *Hint: The `get_min_errs` function will be helpful*. There is a spot below for you to submit your results for each challenge.\n",
    "\n",
    "- Booth function\n",
    "  - $x_0=[8, 9], \\epsilon_x=10^{-7}, \\epsilon_f=10^{-14}$\n",
    "- Beale function\n",
    "  - $x_0=[3, 4], \\epsilon_x=0.5, \\epsilon_f=0.07$\n",
    "- Rosenbrock function\n",
    "  - $x_0=[8, 9], \\epsilon_x=10^{-7}, \\epsilon_f=10^{-14}$\n",
    "- Ackley function\n",
    "  - $x_0=[25, 20], \\epsilon_x=2\\cdot10^{-4}, \\epsilon_f=5\\cdot10^{-4}$. This function is hard. Tell us what you tried and how far you got."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_(t):\n",
    "    if t < 5:\n",
    "        return 1e-4\n",
    "    elif t < 10:\n",
    "        return 1e-2\n",
    "    else:\n",
    "        return 0.1\n",
    "ans.set_settings(fn_name='booth', method='adam', x0=np.array([3, 4]), lr=lr_, num_iters=30, tol=1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How close the optimization got to the red star, and how far the min loss was from the global min.\n",
    "print(ans.get_min_errs())\n",
    "\n",
    "# The gradient at the end of the optimization. This can be helpful for tuning your learning rate schedule!\n",
    "last_grad = ans.grad_val(ans.get_xs_losses()[0][-1])\n",
    "print(last_grad)\n",
    "\n",
    "# Some visuals\n",
    "ans.plot2d()\n",
    "ans.path2d()\n",
    "#ans.video3d()  # This saves a video to the folder this notebook is in!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans.compare('momentum', lr=lambda t: 1e-4, num_iters=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\">Submission: Challenge</span>\n",
    "<span style=\"color:red\">Place code in the below cells that demonstrates your results for each challenge. Each cell should end with `get_min_errs()` displaying the achieved error.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Booth function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace parameters here\n",
    "params = dict(\n",
    "    method='momentum',\n",
    "    lr=lr_,\n",
    "    num_iters=15,\n",
    "    alpha=0.8\n",
    ")\n",
    "ans.set_settings(fn_name='booth', x0=np.array([8, 9]), **params)\n",
    "ans.get_min_errs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beale function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace parameters here\n",
    "params = dict(\n",
    "    method='adagrad',\n",
    "    lr=lr_,\n",
    "    num_iters=15,\n",
    "    alpha=0.8\n",
    ")\n",
    "ans.set_settings(fn_name='beale', x0=np.array([8, 9]), **params)\n",
    "ans.get_min_errs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rosenbrock function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace parameters here\n",
    "params = dict(\n",
    "    method='adagrad',\n",
    "    lr=lr_,\n",
    "    num_iters=10,\n",
    ")\n",
    "ans.set_settings(fn_name='rosen2d', x0=np.array([2, 9]), **params)\n",
    "ans.get_min_errs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ackley function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace parameters here\n",
    "params = dict(\n",
    "    method='adam',\n",
    "    lr=lr_,\n",
    "    num_iters=15,\n",
    ")\n",
    "ans.set_settings(fn_name='ackley2d', x0=np.array([25, 20]), **params)\n",
    "ans.get_min_errs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\">Submission: Project Report</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">Create a report explaining the avenues you explored after the implementation phase of the project, the process you used to select the function values for each combination of functions and initial points, and what you found or learned. You are encouraged to include explanatory images or links to videos generated in the process, showcasing the process you describe or any interesting or unusual phenomena you observe over the course of your investigation!</span>\n",
    "\n",
    "Please append a PDF print-out of this Jupyter notebook, including any code for extensions you choose to do (see below), to your project report."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rubric\n",
    "\n",
    "- A C level project would successfully implement 4 out of 5 of the optimization algorithms and 3 out of 4 of the benchmark functions (and their derivatives).\n",
    "- A B level project would successfully implement all the optimization algorithms and benchmark functions (and their derivatives), as well as complete a project report.\n",
    "- An A level project would do all of the above, plus one or more extensions (we suggest some below, but you can pick anything of sufficient interest and complexity).\n",
    "\n",
    "Note that exceptional projects that go above and beyond may receive extra credit beyond at our discretion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extensions\n",
    "\n",
    "Some extensions to this project you could do and include in your project report are:\n",
    "- Correctly implement more classes of optimization algorithms -- do at least one algorithm for credit, and benchmark its performance. Examples include [proximal gradient descent](http://www.seas.ucla.edu/~vandenbe/236C/lectures/proxgrad.pdf), and [gradient descent with line search](https://optimization.cbe.cornell.edu/index.php?title=Line_search_methods). \n",
    "- A \"literature review\" of optimization algorithms. What this means is to take a research paper or two from the study of optimization algorithms and summarize it (or them) in a way that your peers can understand. A good literature review should contain: an introduction to the proposed method, a formal description of the proposed method, and discussion about why the method is useful or needed. You can use any papers from this field that you please, but here are a few in case you are stuck: [\"Nesterov's Accelerated Gradient and Momentum as approximations to Regularised Update Descent\"](https://arxiv.org/abs/1607.01981), [\"A Universal Catalyst for First-Order Optimization\"](https://arxiv.org/abs/1506.02186). You could also discuss one of the papers your TA Tarun wrote: [\"A Potential Reduction Inspired Algorithm for Exact Max Flow in Almost O(m^(4/3)) Time\"](https://arxiv.org/abs/2009.03260), which has interesting optimization ideas inside an algorithmic framework.\n",
    "- Describe how the algorithms discussed compare to higher-order algorithms, such as Newton's method and especially interior point methods. For interior point methods, implementation will be hard due to numerical stability issues, so you can do a more theoretical review; but other comparisons should involve implementations of the comparison methods.\n",
    "- Quantitatively compare how the dimensionality of the problem can affect the algorithms. How do the different algorithms fare in higher dimensions? What benchmarks or visualizations can you use for higher-dimensional optimization?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
